The main appeal of Bayesian models is that they provide a rigorous framework for making assumptions. In cases where you have no ideas about the data going in (uninformed prior), they are the same. If you have a large number of samples, Bayesian and frequentist methods will converge. But Bayesian methods let you go in with assumptions about what’s more likely which is very helpful when you are dealing with a small number of data points.

A simple example is a coin flip. You might flip a coin you find on the street 5 times and get all heads. Do you think the coin is 100% likely to land heads? Probably not. There are way more fair coins in the world than unfair coins. If you flip it 1000 times and get 1000 heads, even a very strong prior assumption that it’s fair would concede that it’s impossibly unlikely for a fair coin to produce these results. Replace the coin with any hypothesis where you know something about the sample going in, and you can see why it’s helpful to have priors for small datasets.

Even if you have no idea about something, you have enough knowledge about the world around you that some outcomes would seem exceedingly implausible compared to others. I know nothing about snakes or their lifespans, but if I were to create a model to predict a snake's lifespan, i wouldn't think that a 1000 year lifespan is just as plausible as a 10 year lifespan.
