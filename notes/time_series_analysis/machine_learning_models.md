# Machine Learning Models

When it comes to forecasting time series data, selecting the right machine learning model can significantly impact the accuracy and reliability of predictions. These notes provide a comprehensive overview of some of the most commonly used algorithms in the field, each with its unique strengths and limitations.

The table below compares several time series forecasting algorithms, highlighting their descriptions, advantages, disadvantages, and whether they can be applied locally or globally:

| Algorithm Name | Description | Pros | Cons | Local vs Global |
|----------------|-------------|------|------|-----------------|
| **ARIMA (AutoRegressive Integrated Moving Average)** | A statistical model used for analyzing and forecasting time series data by using the dependencies between an observation and a number of lagged observations. | - Flexible and capable of handling a wide range of time series patterns. <br> - Suitable for univariate time series data. | - Complex to understand and implement. <br> - Requires the data to be stationary. <br> - Sensitive to the chosen parameters. <br> - AutoARIMA can alleviate some implementation challenges but still requires expertise. | Local only. Cannot be used globally. |
| **Prophet** | Developed by Facebook, this algorithm is tailored for forecasting time series data with daily observations that include strong seasonal effects and the presence of outliers. | - Easy to use with intuitive parameter settings. <br> - Automatically handles missing data and outliers well. <br> - Provides a simple and interpretable result. | - Less effective for non-daily data or data without strong seasonality. <br> - Requires domain knowledge to fine-tune accurately. <br> - Has faced criticism after issues with high-profile predictions (e.g., Zillow collapse). | Local only. Cannot be used globally. |
| **LSTM (Long Short-Term Memory)** | A type of recurrent neural network (RNN) that is well-suited for learning from sequences and time series data, capable of capturing long-term dependencies. | - Excellent at capturing long-term dependencies and patterns in time series data. <br> - Can handle large and complex datasets. | - Requires substantial amounts of data for training. <br> - Computationally intensive and can be slow to train. <br> - Complex architecture that requires careful tuning of hyperparameters. | Can be used locally or globally. |
| **Holt-Winters Method** | A time series forecasting method that accounts for level, trend, and seasonality by applying exponential smoothing. | - Good for data with trend and seasonal patterns. <br> - Straightforward and relatively easy to implement. | - May not perform well on non-seasonal data. <br> - Sensitive to parameter choices and initial settings. | Local only. Cannot be used globally. |
| **SARIMA (Seasonal ARIMA)** | An extension of ARIMA that includes seasonal components, enabling it to handle seasonal effects in the data. | - Capable of handling both trend and seasonality in the data. <br> - Flexible model structure that can be tailored to specific time series characteristics. | - Complex to configure and requires a thorough understanding of time series analysis. <br> - Data must be stationary, requiring transformations. <br> - AutoARIMA can assist but still demands expertise. | Local only. Cannot be used globally. |
| **Exponential Smoothing** | A forecasting technique that applies weighted averages to past observations, with the weights decaying exponentially over time. | - Simple to implement and use. <br> - Effective for data without clear trend or seasonal patterns. | - May not be accurate for more complex data involving trends and seasonality. <br> - Struggles with data exhibiting sudden changes or volatility. | Local only. Cannot be used globally. |
| **Random Forest** | An ensemble learning method using multiple decision trees to improve predictive performance and robustness. | - Handles a wide variety of data types and is robust to outliers. <br> - Can detect complex interactions and dependencies in the data. | - Computationally intensive, especially for large datasets. <br> - Can overfit if not properly tuned. <br> - Does not extrapolate beyond the range of training data. | Can be used locally or globally. |
| **XGBoost** | A highly efficient and scalable implementation of gradient boosting, particularly effective for structured data and competitions. | - High performance with excellent predictive power. <br> - Handles a wide range of data types well, including complex seasonality. <br> - Offers extensive tuning options and regularization techniques to improve accuracy. | - Can be complex to tune and requires careful parameter selection to avoid overfitting. <br> - Computationally demanding. <br> - Cannot predict values outside the range of training data (above max or below min). | Can be used locally or globally. |
